{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "import jieba\n",
    "\n",
    "import os.path\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import gensim \n",
    "\n",
    "from gensim.models import Word2Vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## data read #####################################\n",
    "\n",
    "#工作空间设置\n",
    "\n",
    "data_path = '/Users/hubei/Downloads/BDcloud下载/Database Castle药物分子/晶泰科技比赛数据/'\n",
    "\n",
    "os.chdir(data_path)#设置当前工作空间\n",
    "\n",
    "print (os.getcwd())#获得当前工作目录\n",
    "\n",
    "\n",
    "\n",
    "#数据读取\n",
    "\n",
    "df_protein_train    = pd.read_csv('df_protein_train.csv')#1653\n",
    "\n",
    "df_protein_test     = pd.read_csv('df_protein_test.csv')#414\n",
    "\n",
    "protein_concat = pd.concat([df_protein_train,df_protein_test])\n",
    "\n",
    "# print(protein_concat.tail(10))\n",
    "\n",
    "df_molecule         = pd.read_csv('df_molecule.csv')#111216\n",
    "\n",
    "df_affinity_train   = pd.read_csv('df_affinity_train.csv')#165084\n",
    "\n",
    "df_affinity_test    = pd.read_csv('df_affinity_test_toBePredicted.csv')#41383\n",
    "\n",
    "df_affinity_test['Ki'] = -11\n",
    "\n",
    "data  =  pd.concat([df_affinity_train,df_affinity_test])\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########                                 feature                               ############\n",
    "###############################################################################################\n",
    "\n",
    "#1、Fingerprint分子指纹处理展开\n",
    "\n",
    "feat = []\n",
    "\n",
    "for i in range(0,len(df_molecule)):\n",
    "\n",
    "    feat.append(df_molecule['Fingerprint'][i].split(','))\n",
    "\n",
    "feat = pd.DataFrame(feat)\n",
    "\n",
    "# print(feat.head())\n",
    "\n",
    "feat = feat.astype('int')\n",
    "\n",
    "feat.columns=[\"Fingerprint_{0}\".format(i) for i in range(0,167)]\n",
    "\n",
    "# print(feat.head())\n",
    "\n",
    "feat[\"Molecule_ID\"] = df_molecule['Molecule_ID']\n",
    "\n",
    "# print(feat.head())\n",
    "\n",
    "data = data.merge(feat, on='Molecule_ID', how='left')\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2、df_molecule其他特征处理\n",
    "\n",
    "feat = df_molecule.drop('Fingerprint',axis=1)          #删除Fingerprint列\n",
    "\n",
    "# print(feat.head())\n",
    "\n",
    "data = data.merge(feat, on='Molecule_ID', how='left')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3、protein 蛋白质 词向量训练 看不懂的加公众号：Python_R_wu，稍后会讲解下，来不及写\n",
    "\n",
    "n = 128\n",
    "\n",
    "texts = [[word for word in re.findall(r'.{3}',document)]  \n",
    "         # re.findall:在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。\n",
    "\n",
    "               for document in list(protein_concat['Sequence'])]\n",
    "\n",
    "# print(texts)\n",
    "\n",
    "model = Word2Vec(texts,size=n,window=4,min_count=1,negative=3,\n",
    "\n",
    "                 sg=1,sample=0.001,hs=1,workers=4)          # sg ---> skip-gram模型\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.DataFrame([model[word] for word in (model.wv.vocab)])\n",
    "\n",
    "print(vectors.head(6))\n",
    "\n",
    "vectors['Word'] = list(model.wv.vocab)\n",
    "\n",
    "vectors.columns= [\"vec_{0}\".format(i) for i in range(0,n)]+[\"Word\"]\n",
    "vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_vec = pd.DataFrame()\n",
    "\n",
    "result1=[]\n",
    "\n",
    "aa = list(protein_concat['Protein_ID'])\n",
    "# print(texts) \n",
    "# texts是蛋白质序列 3 分词之后的数组\n",
    "\n",
    "for i in range(len(texts)):\n",
    "\n",
    "    result2=[]         \n",
    "\n",
    "    for w in range(len(texts[i])):\n",
    "\n",
    "        result2.append(aa[i])    \n",
    "\n",
    "    result1.extend(result2)\n",
    "    \n",
    "wide_vec['Id'] = result1\n",
    "\n",
    "wide_vec.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=[]\n",
    "\n",
    "for i in range(len(texts)):\n",
    "\n",
    "    result2=[]         \n",
    "\n",
    "    for w in range(len(texts[i])):\n",
    "\n",
    "        result2.append(texts[i][w])    \n",
    "\n",
    "    result1.extend(result2)\n",
    "\n",
    "wide_vec['Word'] = result1\n",
    "\n",
    "wide_vec.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result1,result2\n",
    "\n",
    "\n",
    "wide_vec = wide_vec.merge(vectors,on='Word', how='left')\n",
    "\n",
    "print(wide_vec.head())\n",
    "\n",
    "wide_vec = wide_vec.drop('Word',axis=1)\n",
    "\n",
    "wide_vec.columns = ['Protein_ID']+[\"vec_{0}\".format(i) for i in range(0,n)]\n",
    "\n",
    "wide_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectors\n",
    "\n",
    "\n",
    "\n",
    "name = [\"vec_{0}\".format(i) for i in range(0,n)]\n",
    "\n",
    "\n",
    "\n",
    "feat = pd.DataFrame(wide_vec.groupby(['Protein_ID'])[name].agg('mean')).reset_index()\n",
    "# 按蛋白质ID分组后将每个ID按平均值聚集\n",
    "print(feat.head())\n",
    "\n",
    "feat.columns=[\"Protein_ID\"]+[\"mean_ci_{0}\".format(i) for i in range(0,n)]\n",
    "\n",
    "data = data.merge(feat, on='Protein_ID', how='left')\n",
    "data.head()\n",
    "\n",
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#################################### lgb ############################\n",
    "\n",
    "train_feat = data[data['Ki']> -11].fillna(0)\n",
    "\n",
    "testt_feat = data[data['Ki']<=-11].fillna(0)\n",
    "\n",
    "label_x  = train_feat['Ki']\n",
    "\n",
    "label_y  = testt_feat['Ki']\n",
    "\n",
    "\n",
    "\n",
    "submission = testt_feat[['Protein_ID','Molecule_ID']]\n",
    "\n",
    "len(testt_feat)\n",
    "\n",
    "train_feat = train_feat.drop('Ki',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Ki',axis=1)\n",
    "\n",
    "train_feat = train_feat.drop('Protein_ID',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Protein_ID',axis=1)\n",
    "\n",
    "train_feat = train_feat.drop('Molecule_ID',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Molecule_ID',axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#lgb算法\n",
    "\n",
    "train = lgb.Dataset(train_feat, label=label_x)\n",
    "\n",
    "test  = lgb.Dataset(testt_feat, label=label_y,reference=train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''params = {\n",
    "\n",
    "    'boosting_type': 'gbdt',\n",
    "\n",
    "    'objective': 'regression_l2',\n",
    "\n",
    "    'metric': 'l2',\n",
    "\n",
    "    #'objective': 'multiclass',\n",
    "\n",
    "    #'metric': 'multi_error',\n",
    "\n",
    "    #'num_class':5,\n",
    "\n",
    "    'min_child_weight': 3,\n",
    "\n",
    "    'num_leaves': 2 ** 5,\n",
    "\n",
    "    'lambda_l2': 10,\n",
    "\n",
    "    'subsample': 0.7,\n",
    "\n",
    "    'colsample_bytree': 0.7,\n",
    "\n",
    "    'colsample_bylevel': 0.7,\n",
    "\n",
    "    'learning_rate': 0.05,\n",
    "\n",
    "    'tree_method': 'exact',\n",
    "\n",
    "    'seed': 2017,\n",
    "\n",
    "    'nthread': 12,\n",
    "\n",
    "    'silent': True\n",
    "\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''num_round = 1000\n",
    "\n",
    "gbm = lgb.train(params, \n",
    "\n",
    "                  train, \n",
    "\n",
    "                  num_round, \n",
    "\n",
    "                  verbose_eval=50,\n",
    "\n",
    "                  valid_sets=[train,test]\n",
    "\n",
    "                  )\n",
    " \n",
    "\n",
    "\n",
    "preds_sub = gbm.predict(testt_feat)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"################ xgb ####################\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# 读入数据\n",
    "\n",
    "train_feat = data[data['Ki']> -11].fillna(0)\n",
    "\n",
    "testt_feat = data[data['Ki']<=-11].fillna(0)\n",
    "\n",
    "label_x  = train_feat['Ki']\n",
    "\n",
    "label_y  = testt_feat['Ki']\n",
    "\n",
    "submission = testt_feat[['Protein_ID','Molecule_ID']]\n",
    "\n",
    "train_feat = train_feat.drop('Ki',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Ki',axis=1)\n",
    "\n",
    "train_feat = train_feat.drop('Protein_ID',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Protein_ID',axis=1)\n",
    "\n",
    "train_feat = train_feat.drop('Molecule_ID',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Molecule_ID',axis=1)\n",
    "\n",
    "\n",
    "xgb_train = xgb.DMatrix(train_feat,label = label_x)\n",
    "\n",
    "xgb_test = xgb.DMatrix(testt_feat,label = label_y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "params = { \n",
    "\n",
    "    'gamma':0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    \n",
    "    'max_depth':6, # 构建树的深度，越大越容易过拟合\n",
    "    \n",
    "    'lambda':0.5,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    \n",
    "    'subsample':0.8, # 随机采样训练样本\n",
    "    \n",
    "    'colsample_bytree':0.7, # 生成树时进行的列采样\n",
    "    \n",
    "    'min_child_weight':3, \n",
    "    \n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 \n",
    "    \n",
    "    'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.\n",
    "    \n",
    "    'eta': 0.07, # 如同学习率\n",
    "    \n",
    "    'seed':2018,\n",
    "    \n",
    "    #'nthread':0,# cpu 线程数\n",
    "    \n",
    "    #'eval_metric': 'auc'\n",
    "\n",
    "}\n",
    "\n",
    "#plst = list(params.items())\n",
    "\n",
    "num_round = 10000 # 迭代次数\n",
    "evallist=[(xgb_test,'test'), (xgb_train, 'train')]\n",
    "\n",
    "model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=num_round, evals=evallist, verbose_eval=50,) \n",
    "\n",
    "preds_sub = model.predict(xgb.DMatrix(testt_feat, label=label_y))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#结果保存\n",
    "\n",
    "nowTime=datetime.datetime.now().strftime('%m%d%H%M')#现在\n",
    "\n",
    "# name='lgb_'+nowTime+'.csv'\n",
    "\n",
    "name='xgb_'+nowTime+'.csv'\n",
    "\n",
    "submission['Ki'] = preds_sub\n",
    "\n",
    "submission.to_csv(name, index=False)\n",
    "\n",
    "#输出运行时长\n",
    "\n",
    "cost_time = time.time()-start_time\n",
    "\n",
    "print (\"xgboost success!\",'\\n',\"cost time:\",cost_time,\"(s)......\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### 逻辑回归 ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat = data[data['Ki']> -11].fillna(0)\n",
    "\n",
    "testt_feat = data[data['Ki']<=-11].fillna(0)\n",
    "\n",
    "label_x  = train_feat['Ki']\n",
    "\n",
    "label_y  = testt_feat['Ki']\n",
    "\n",
    "submission = testt_feat[['Protein_ID','Molecule_ID']]\n",
    "\n",
    "train_feat = train_feat.drop('Ki',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Ki',axis=1)\n",
    "\n",
    "train_feat = train_feat.drop('Protein_ID',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Protein_ID',axis=1)\n",
    "\n",
    "train_feat = train_feat.drop('Molecule_ID',axis=1)\n",
    "\n",
    "testt_feat = testt_feat.drop('Molecule_ID',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "encoded = le.fit_transform(label_x)\n",
    "cls = LogisticRegression()\n",
    "cls.fit(X = train_feat,y = encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
